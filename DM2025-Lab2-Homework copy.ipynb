{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Student Information**\n",
    "Name: ÁéãÂìÅÁø∞\n",
    "\n",
    "Student ID: 111034026\n",
    "\n",
    "GitHub ID:\n",
    "\n",
    "Kaggle name:\n",
    "\n",
    "Kaggle private scoreboard snapshot: \n",
    "\n",
    "![pic_ranking.png](./pics/pic_ranking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Instructions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we have divided the assignments into **three phases/parts**. The `first two phases` refer to the `exercises inside the Master notebooks` of the [DM2025-Lab2-Exercise Repo](https://github.com/difersalest/DM2025-Lab2-Exercise.git). The `third phase` refers to an `internal Kaggle competition` that we are gonna run among all the Data Mining students. Together they add up to `100 points` of your grade. There are also some `bonus points` to be gained if you complete `extra exercises` in the lab **(bonus 15 pts)** and in the `Kaggle Competition report` **(bonus 5 pts)**.\n",
    "\n",
    "**Environment recommendations to solve lab 2:**\n",
    "- **Phase 1 exercises:** Need GPU for training the models explained in that part, if you don't have a GPU in your laptop it is recommended to run in Colab or Kaggle for a faster experience, although with CPU they can still be solved but with a slower execution.\n",
    "- **Phase 2 exercises:** We use Gemini's API so everything can be run with only CPU without a problem.\n",
    "- **Phase 3 exercises:** For the competition you will probably need GPU to train your models, so it is recommended to use Colab or Kaggle if you don't have a laptop with a dedicated GPU.\n",
    "- **Optional Ollama Notebook (not graded):** You need GPU, at least 4GB of VRAM with 16 GB of RAM to run the local open-source LLM models. \n",
    "\n",
    "## **Phase 1 (30 pts):**\n",
    "\n",
    "1. __Main Exercises (25 pts):__ Do the **take home exercises** from Sections: `1. Data Preparation` to `9. High-dimension Visualization: t-SNE and UMAP`, in the [DM2025-Lab2-Master-Phase_1 Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_1.ipynb). Total: `8 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 3th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "## **Phase 2 (30 pts):**\n",
    "\n",
    "1. **Main Exercises (25 pts):** Do the remaining **take home exercises** from Section: `2. Large Language Models (LLMs)` in the [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb). Total: `5 exercises required from sections 2.1, 2.2, 2.4 and 2.6`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "3. **`Bonus (15 pts):`** Complete the bonus exercises in the [DM2025-Lab2-Master-Phase_2_Bonus Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Bonus.ipynb) and [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb) `where 2 exercises are counted as bonus from sections 2.3 and 2.5 in the main notebook`. Total: `7 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "## **Phase 3 (40 pts):**\n",
    "\n",
    "1. **Kaggle Competition Participation (30 pts):** Participate in the in-class **Kaggle Competition** regarding Emotion Recognition on Twitter by clicking in this link: **[Data Mining Class Kaggle Competition](https://www.kaggle.com/t/3a2df4c6d6b4417e8bf718ed648d7554)**. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20 pts of the 30 pts in this competition participation part.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**. Make sure to take a screenshot of your position at the end of the competition and store it as `pic_ranking.png` under the `pics` folder of this repository and rerun the cell **Student Information**.\n",
    "\n",
    "2. **Competition Report (10 pts)** A report section to be filled in inside this notebook in Markdown Format, we already provided you with the template below. You need to describe your work developing the model for the competition. The report should include a section describing briefly the following elements: \n",
    "* Your preprocessing steps.\n",
    "* The feature engineering steps.\n",
    "* Explanation of your model.\n",
    "\n",
    "* **`Bonus (5 pts):`**\n",
    "    * You will have to describe more detail in the previous steps.\n",
    "    * Mention different things you tried.\n",
    "    * Mention insights you gained. \n",
    "\n",
    "[Markdown Guide - Basic Syntax](https://www.markdownguide.org/basic-syntax/)\n",
    "\n",
    "**`Things to note for Phase 3:`**\n",
    "\n",
    "* **The code used for the competition should be in this Jupyter Notebook File** `DM2025-Lab2-Homework.ipynb`.\n",
    "\n",
    "* **Push the code used for the competition to your repository**.\n",
    "\n",
    "* **The code should have a clear separation for the same sections of the report, preprocessing, feature engineering and model explanation. Briefly comment your code for easier understanding, we provide a template at the end of this notebook.**\n",
    "\n",
    "* Showing the kaggle screenshot of the ranking plus the code in this notebook will ensure the validity of your participation and the report to obtain the corresponding points.\n",
    "\n",
    "After the competition ends you will have two days more to submit the `DM2025-Lab2-Homework.ipynb` with your report in markdown format and your code. Do everything **`BEFORE the deadline (Nov. 26th, 11:59 pm, Wednesday) to obtain 100% of the available points.`**\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding NTU Cool assignment.\n",
    "\n",
    "## **Deadlines:**\n",
    "\n",
    "![lab2_deadlines](./pics/lab2_deadlines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next you will find the template report with some simple markdown syntax explanations, use it to structure your content.\n",
    "\n",
    "You can delete the syntax suggestions after you use them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# **Project Report**\n",
    "\n",
    "**Syntax:** `#` creates the largest heading (H1).\n",
    "\n",
    "---\n",
    "**Syntax:** `---` creates a horizontal rule (a separator line).\n",
    "\n",
    "## 1. Model Development (10 pts Required)\n",
    "\n",
    "**Syntax:** `##` creates a secondary heading (H2).\n",
    "\n",
    "**Describe briefly each section, you can add graphs/charts to support your explanations.**\n",
    "\n",
    "### 1.1 Preprocessing Steps\n",
    "\n",
    "**Syntax:** `###` creates a tertiary heading (H3).\n",
    "\n",
    "[Content for Preprocessing]\n",
    "\n",
    "**Example Syntax for Content:**\n",
    "*   **Bold text:** `**text**`\n",
    "*   *Italic text*: `*text*`\n",
    "*   Bullet point list:\n",
    "    * Item 1\n",
    "    * Item 2\n",
    "\n",
    "Markdown Syntax to Add Image: `![Description of the Image](./your_local_folder/name_of_the_image.png)`\n",
    "\n",
    "![Example Markdown Syntax to Add Image](./pics/example_md_img.png)\n",
    "\n",
    "### 1.2 Feature Engineering Steps\n",
    "\n",
    "[Content for Feature Engineering]\n",
    "\n",
    "### 1.3 Explanation of Your Model\n",
    "\n",
    "[Content for Model Explanation]\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Bonus Section (5 pts Optional)\n",
    "\n",
    "**Add more detail in previous sections**\n",
    "\n",
    "### 2.1 Mention Different Things You Tried\n",
    "\n",
    "[Content for Experiments]\n",
    "\n",
    "### 2.2 Mention Insights You Gained\n",
    "\n",
    "[Content for Insights]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`From here on starts the code section for the competition.`**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Competition Code**\n",
    "\n",
    "## 1. Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Parsed posts_df:\n",
      "         id                                               text\n",
      "0  0x61fc95  We got the ranch, loaded our guns and sat up t...\n",
      "1  0x35663e  I bet there is an army of married couples who ...\n",
      "2  0xc78afe                         This could only end badly.\n",
      "3  0x90089c  My sister squeezed a lime in her milk when she...\n",
      "4  0xaba820         and that got my head bobbing a little bit.\n",
      "posts_df shape: (64171, 2)\n",
      "Emotion columns: ['id', 'emotion']\n",
      "Identification columns: ['id', 'split']\n",
      "Merged df shape: (64171, 4)\n",
      "         id                                               text  split emotion\n",
      "0  0x61fc95  We got the ranch, loaded our guns and sat up t...   test     NaN\n",
      "1  0x35663e  I bet there is an army of married couples who ...  train     joy\n",
      "2  0xc78afe                         This could only end badly.  train    fear\n",
      "3  0x90089c  My sister squeezed a lime in her milk when she...  train     joy\n",
      "4  0xaba820         and that got my head bobbing a little bit.   test     NaN\n",
      "Train size: (47890, 4)\n",
      "Test size: (16281, 4)\n",
      "Cleaning text...\n",
      "                                                text  \\\n",
      "1  I bet there is an army of married couples who ...   \n",
      "2                         This could only end badly.   \n",
      "3  My sister squeezed a lime in her milk when she...   \n",
      "7                                Thank you so much‚ù§Ô∏è   \n",
      "9  Stinks because ive been in this program for a ...   \n",
      "\n",
      "                                          clean_text  \n",
      "1  i bet there is an army of married couples who ...  \n",
      "2                          this could only end badly  \n",
      "3  my sister squeezed a lime in her milk when she...  \n",
      "7                                  thank you so much  \n",
      "9  stinks because ive been in this program for a ...  \n"
     ]
    }
   ],
   "source": [
    "### Add the code related to the preprocessing steps in cells inside this section\n",
    "# ================================\n",
    "# 1. Preprocessing Steps\n",
    "# ================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Â¶ÇÊûú‰Ω†Âú®Êú¨Ê©ü / ColabÔºåË¶ÅË®òÂæóÁ¢∫ÂÆöË∑ØÂæëÊ≠£Á¢∫\n",
    "DATA_DIR = \"./kaggle_data\"\n",
    "\n",
    "# Ê™îÊ°àË∑ØÂæë\n",
    "posts_path = os.path.join(DATA_DIR, \"final_posts.json\")\n",
    "emotion_path = os.path.join(DATA_DIR, \"emotion.csv\")\n",
    "ident_path = os.path.join(DATA_DIR, \"data_identification.csv\")\n",
    "\n",
    "print(\"Loading data...\")\n",
    "\n",
    "# ===============================\n",
    "# Ê≠£Á¢∫ËÆÄÂèñ final_posts.jsonÔºàÂ∑¢ÁãÄ JSONÔºâ\n",
    "# ===============================\n",
    "with open(posts_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)  # data ÊòØ listÔºåÊØèÁ≠ÜË≥áÊñôÊúâ‰∏ÄÂÄã root Ê¨Ñ‰Ωç\n",
    "\n",
    "records = []\n",
    "\n",
    "for item in data:\n",
    "    try:\n",
    "        post_obj = item[\"root\"][\"_source\"][\"post\"]\n",
    "        post_id = post_obj.get(\"post_id\")\n",
    "        text = post_obj.get(\"text\")\n",
    "        records.append({\"id\": post_id, \"text\": text})\n",
    "    except KeyError as e:\n",
    "        print(\"Skipping due to missing key:\", e)\n",
    "\n",
    "posts_df = pd.DataFrame(records)\n",
    "\n",
    "print(\"Parsed posts_df:\")\n",
    "print(posts_df.head())\n",
    "print(\"posts_df shape:\", posts_df.shape)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# ËºâÂÖ•ÂÖ∂‰ªñÂÖ©ÂÄã csv\n",
    "# ===============================\n",
    "emotion_df = pd.read_csv(emotion_path)\n",
    "print(\"Emotion columns:\", emotion_df.columns.tolist())\n",
    "\n",
    "ident_df = pd.read_csv(ident_path)\n",
    "print(\"Identification columns:\", ident_df.columns.tolist())\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# ‰æùÁÖß id Âêà‰Ωµ\n",
    "# ===============================\n",
    "df = posts_df.merge(ident_df, on=\"id\", how=\"inner\")\n",
    "df = df.merge(emotion_df, on=\"id\", how=\"left\")\n",
    "\n",
    "print(\"Merged df shape:\", df.shape)\n",
    "print(df.head())\n",
    "\n",
    "assert \"text\" in df.columns\n",
    "assert \"split\" in df.columns\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# ÂàÜ train / test\n",
    "# ===============================\n",
    "train_df = df[df[\"split\"] == \"train\"].copy()\n",
    "test_df = df[df[\"split\"] == \"test\"].copy()\n",
    "\n",
    "print(\"Train size:\", train_df.shape)\n",
    "print(\"Test size:\", test_df.shape)\n",
    "\n",
    "train_df[\"text\"] = train_df[\"text\"].fillna(\"\")\n",
    "test_df[\"text\"] = test_df[\"text\"].fillna(\"\")\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# ÊñáÂ≠óÊ∏ÖÁêÜ\n",
    "# ===============================\n",
    "url_pattern = re.compile(r\"http\\S+|www\\.\\S+\")\n",
    "user_pattern = re.compile(r\"@\\w+\")\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = url_pattern.sub(\" \", text)\n",
    "    text = user_pattern.sub(\" \", text)\n",
    "    text = re.sub(r\"#\", \" \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "print(\"Cleaning text...\")\n",
    "\n",
    "train_df[\"clean_text\"] = train_df[\"text\"].apply(clean_text)\n",
    "test_df[\"clean_text\"] = test_df[\"text\"].apply(clean_text)\n",
    "\n",
    "print(train_df[[\"text\", \"clean_text\"]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hansw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running enhanced preprocessing...\n",
      "                                                text  \\\n",
      "1  I bet there is an army of married couples who ...   \n",
      "2                         This could only end badly.   \n",
      "3  My sister squeezed a lime in her milk when she...   \n",
      "7                                Thank you so much‚ù§Ô∏è   \n",
      "9  Stinks because ive been in this program for a ...   \n",
      "\n",
      "                                              clean2  \n",
      "1  i bet there is an army of married couple who d...  \n",
      "2                          this could only end badly  \n",
      "3  my sister squeezed a lime in her milk when she...  \n",
      "7                                  thank you so much  \n",
      "9  stink because ive been in this program for a y...  \n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# TEXT PREPROCESSING (Âº∑ÂåñÁâà)\n",
    "# ================================\n",
    "import re\n",
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# üî• 1. Contraction Expansion\n",
    "contractions = {\n",
    "    \"can't\": \"can not\", \"won't\": \"will not\", \"n't\": \" not\",\n",
    "    \"i'm\": \"i am\", \"it's\": \"it is\", \"he's\": \"he is\", \"she's\": \"she is\",\n",
    "    \"they're\": \"they are\", \"we're\": \"we are\", \"you're\": \"you are\",\n",
    "    \"i've\": \"i have\", \"we've\": \"we have\", \"they've\": \"they have\",\n",
    "    \"isn't\": \"is not\", \"aren't\": \"are not\", \"wasn't\": \"was not\",\n",
    "    \"weren't\": \"were not\"\n",
    "}\n",
    "\n",
    "def expand_contractions(text):\n",
    "    for c, e in contractions.items():\n",
    "        text = re.sub(rf\"\\b{c}\\b\", e, text)\n",
    "    return text\n",
    "\n",
    "# üî• 2. Emoji ‚Üí Emotion\n",
    "emoji_map = {\n",
    "    \"üòÄ\":\"joy\", \"üòÉ\":\"joy\", \"üòÑ\":\"joy\", \"üòÅ\":\"joy\", \"üòÜ\":\"joy\",\n",
    "    \"üòÇ\":\"joy\", \"üòä\":\"joy\", \"üôÇ\":\"joy\", \"üòç\":\"joy\",\n",
    "\n",
    "    \"üò°\":\"anger\", \"üò†\":\"anger\",\n",
    "\n",
    "    \"üò¢\":\"sadness\", \"üò≠\":\"sadness\",\n",
    "\n",
    "    \"üò±\":\"fear\", \"üò®\":\"fear\",\n",
    "\n",
    "    \"ü§¢\":\"disgust\", \"ü§Æ\":\"disgust\",\n",
    "\n",
    "    \"üòÆ\":\"surprise\", \"üòØ\":\"surprise\", \"üò≤\":\"surprise\"\n",
    "}\n",
    "\n",
    "def replace_emojis(text):\n",
    "    for e, label in emoji_map.items():\n",
    "        text = text.replace(e, f\" {label} \")\n",
    "    return text\n",
    "\n",
    "# üî• 3. Negation Handling\n",
    "def join_negations(text):\n",
    "    return re.sub(r\"(not|no|never)\\s+(\\w+)\", r\"\\1_\\2\", text)\n",
    "\n",
    "# üî• 4. Lemmatization\n",
    "def lemmatize(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(w) for w in text.split()])\n",
    "\n",
    "# üî• 5. full cleaning\n",
    "def full_clean(text):\n",
    "    text = text.lower()\n",
    "    text = expand_contractions(text)\n",
    "    text = replace_emojis(text)\n",
    "    text = join_negations(text)\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)\n",
    "    text = re.sub(r\"@\\w+\", \" \", text)\n",
    "    text = re.sub(r\"#\", \" \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s_]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    text = lemmatize(text)\n",
    "    return text\n",
    "\n",
    "print(\"Running enhanced preprocessing...\")\n",
    "\n",
    "train_df[\"clean2\"] = train_df[\"clean_text\"].apply(full_clean)\n",
    "test_df[\"clean2\"] = test_df[\"clean_text\"].apply(full_clean)\n",
    "\n",
    "print(train_df[[\"text\", \"clean2\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying enhanced preprocessing...\n"
     ]
    }
   ],
   "source": [
    "# ================\n",
    "# Preprocessing\n",
    "# ================\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -------- Emoji mapping --------\n",
    "emoji_map = {\n",
    "    \"üòÇ\": \"LAUGH_EMOJI\",\n",
    "    \"ü§£\": \"LAUGH_EMOJI\",\n",
    "    \"üò≠\": \"CRY_EMOJI\",\n",
    "    \"üò¢\": \"CRY_EMOJI\",\n",
    "    \"üò°\": \"ANGRY_EMOJI\",\n",
    "    \"ü§¨\": \"ANGRY_EMOJI\",\n",
    "    \"üò±\": \"FEAR_EMOJI\",\n",
    "    \"üò®\": \"FEAR_EMOJI\",\n",
    "    \"ü§¢\": \"DISGUST_EMOJI\",\n",
    "    \"ü§Æ\": \"DISGUST_EMOJI\",\n",
    "    \"üòÆ\": \"SURPRISE_EMOJI\",\n",
    "    \"üò≤\": \"SURPRISE_EMOJI\",\n",
    "    \"üòÉ\": \"JOY_EMOJI\",\n",
    "    \"üòä\": \"JOY_EMOJI\",\n",
    "    \"‚ù§Ô∏è\": \"LOVE_EMOJI\"\n",
    "}\n",
    "\n",
    "# -------- Emotion keyword dictionary --------\n",
    "emotion_keywords = {\n",
    "    \"joy\": [\"happy\", \"joy\", \"love\", \"glad\", \"excited\", \"wonderful\", \"awesome\", \"great\"],\n",
    "    \"anger\": [\"angry\", \"furious\", \"hate\", \"mad\", \"pissed\", \"rage\"],\n",
    "    \"sadness\": [\"sad\", \"cry\", \"depressed\", \"lonely\", \"heartbroken\"],\n",
    "    \"fear\": [\"fear\", \"scared\", \"afraid\", \"terrified\", \"panic\"],\n",
    "    \"disgust\": [\"disgust\", \"gross\", \"ew\", \"nasty\"],\n",
    "    \"surprise\": [\"surprise\", \"surprised\", \"omg\", \"wtf\", \"shocked\"],\n",
    "}\n",
    "\n",
    "# -------- Preprocess function --------\n",
    "def preprocess_text(t):\n",
    "    if type(t) != str:\n",
    "        return \"\"\n",
    "\n",
    "    # basic clean\n",
    "    t = t.lower()\n",
    "    t = re.sub(r\"http\\S+|www\\.\\S+\", \" URL \", t)\n",
    "    t = re.sub(r\"@\\w+\", \" USER \", t)\n",
    "\n",
    "    # emoji mapping\n",
    "    for e, token in emoji_map.items():\n",
    "        t = t.replace(e, f\" {token} \")\n",
    "\n",
    "    # keyword inject\n",
    "    for emo, words in emotion_keywords.items():\n",
    "        for w in words:\n",
    "            t = re.sub(rf\"\\b{w}\\b\", f\" {emo.upper()}_WORD \", t)\n",
    "\n",
    "    # remove special chars\n",
    "    t = re.sub(r\"[^a-zA-Z0-9_\\s]\", \" \", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "print(\"Applying enhanced preprocessing...\")\n",
    "train_df[\"clean3\"] = train_df[\"clean2\"].apply(preprocess_text)\n",
    "test_df[\"clean3\"] = test_df[\"clean2\"].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding MiniLM...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dea27659ba4469ea20b6c0f0cb405c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/749 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a99865256bbf439abd3dbb93777bbb62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/255 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniLM shape: (47890, 384)\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# MiniLM Embeddings\n",
    "# ================================\n",
    "!pip install -q sentence-transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "st_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "print(\"Encoding MiniLM...\")\n",
    "\n",
    "X_train_minilm = st_model.encode(\n",
    "    train_df[\"clean3\"].tolist(),\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "X_test_minilm = st_model.encode(\n",
    "    test_df[\"clean3\"].tolist(),\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "y_train = train_df[\"emotion\"].values\n",
    "\n",
    "print(\"MiniLM shape:\", X_train_minilm.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building numeric features...\n",
      "Numeric feature shape: (47890, 11)\n"
     ]
    }
   ],
   "source": [
    "def numeric_features(text):\n",
    "    return [\n",
    "        len(text.split()),              # length\n",
    "        text.count(\"!\"),                # exclamation\n",
    "        text.count(\"?\"),                # question\n",
    "        sum(c.isupper() for c in text), # upper chars\n",
    "        text.count(\"JOY_WORD\"),\n",
    "        text.count(\"ANGER_WORD\"),\n",
    "        text.count(\"FEAR_WORD\"),\n",
    "        text.count(\"SADNESS_WORD\"),\n",
    "        text.count(\"DISGUST_WORD\"),\n",
    "        text.count(\"SURPRISE_WORD\"),\n",
    "        text.count(\"EMOJI\")\n",
    "    ]\n",
    "\n",
    "print(\"Building numeric features...\")\n",
    "\n",
    "numeric_train = np.array([numeric_features(t) for t in train_df[\"clean3\"]])\n",
    "numeric_test = np.array([numeric_features(t) for t in test_df[\"clean3\"]])\n",
    "\n",
    "print(\"Numeric feature shape:\", numeric_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature shape: (47890, 395)\n"
     ]
    }
   ],
   "source": [
    "X_train_full = np.concatenate([X_train_minilm, numeric_train], axis=1)\n",
    "X_test_full = np.concatenate([X_test_minilm, numeric_test], axis=1)\n",
    "\n",
    "print(\"Final feature shape:\", X_train_full.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Implementation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q lightgbm xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.082573 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 98041\n",
      "[LightGBM] [Info] Number of data points in the train set: 40706, number of used features: 392\n",
      "[LightGBM] [Info] Start training from score -1.499201\n",
      "[LightGBM] [Info] Start training from score -3.701388\n",
      "[LightGBM] [Info] Start training from score -3.171052\n",
      "[LightGBM] [Info] Start training from score -0.699357\n",
      "[LightGBM] [Info] Start training from score -2.501303\n",
      "[LightGBM] [Info] Start training from score -2.031337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hansw\\anaconda3\\envs\\dm2025\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM F1: 0.3651423305470703\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got ['anger' 'disgust' 'fear' 'joy' 'sadness' 'surprise']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# --------------------\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# 2) XGBoost\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# --------------------\u001b[39;00m\n\u001b[32m     33\u001b[39m xgb_clf = xgb.XGBClassifier(\n\u001b[32m     34\u001b[39m     max_depth=\u001b[32m6\u001b[39m,\n\u001b[32m     35\u001b[39m     n_estimators=\u001b[32m350\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     39\u001b[39m     nthread=-\u001b[32m1\u001b[39m\n\u001b[32m     40\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[43mxgb_clf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m pred_xgb = xgb_clf.predict(X_val)\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mXGBoost F1:\u001b[39m\u001b[33m\"\u001b[39m, f1_score(y_val, pred_xgb, average=\u001b[33m\"\u001b[39m\u001b[33mmacro\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hansw\\anaconda3\\envs\\dm2025\\Lib\\site-packages\\xgboost\\core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hansw\\anaconda3\\envs\\dm2025\\Lib\\site-packages\\xgboost\\sklearn.py:1761\u001b[39m, in \u001b[36mXGBClassifier.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1756\u001b[39m     expected_classes = \u001b[38;5;28mself\u001b[39m.classes_\n\u001b[32m   1757\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1758\u001b[39m     classes.shape != expected_classes.shape\n\u001b[32m   1759\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (classes == expected_classes).all()\n\u001b[32m   1760\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1761\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1762\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid classes inferred from unique values of `y`.  \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1763\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclasses\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1764\u001b[39m     )\n\u001b[32m   1766\u001b[39m params = \u001b[38;5;28mself\u001b[39m.get_xgb_params()\n\u001b[32m   1768\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.objective):\n",
      "\u001b[31mValueError\u001b[39m: Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got ['anger' 'disgust' 'fear' 'joy' 'sadness' 'surprise']"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ====================================\n",
    "# Label Encoding\n",
    "# ====================================\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y_train)   # convert emotion string ‚Üí int (0~5)\n",
    "\n",
    "print(\"Label classes:\", le.classes_)\n",
    "\n",
    "# ============================\n",
    "# Train/Val split\n",
    "# ============================\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train_full, y_encoded,\n",
    "    test_size=0.15,\n",
    "    random_state=42,\n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "# --------------------\n",
    "# 1) LightGBM\n",
    "# --------------------\n",
    "lgb_clf = lgb.LGBMClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=40,\n",
    "    random_state=42\n",
    ")\n",
    "lgb_clf.fit(X_tr, y_tr)\n",
    "pred_lgb = lgb_clf.predict(X_val)\n",
    "print(\"LightGBM F1:\", f1_score(y_val, pred_lgb, average=\"macro\"))\n",
    "\n",
    "# --------------------\n",
    "# 2) XGBoost\n",
    "# --------------------\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "    max_depth=6,\n",
    "    n_estimators=350,\n",
    "    learning_rate=0.05,\n",
    "    eval_metric=\"mlogloss\",\n",
    "    tree_method=\"hist\",\n",
    "    nthread=-1\n",
    ")\n",
    "xgb_clf.fit(X_tr, y_tr)\n",
    "pred_xgb = xgb_clf.predict(X_val)\n",
    "print(\"XGBoost F1:\", f1_score(y_val, pred_xgb, average=\"macro\"))\n",
    "\n",
    "# --------------------\n",
    "# 3) SVC\n",
    "# --------------------\n",
    "svc_clf = LinearSVC()\n",
    "svc_clf.fit(X_tr, y_tr)\n",
    "pred_svc = svc_clf.predict(X_val)\n",
    "print(\"SVC F1:\", f1_score(y_val, pred_svc, average=\"macro\"))\n",
    "\n",
    "# --------------------\n",
    "# 4) LogisticRegression\n",
    "# --------------------\n",
    "lr_clf = LogisticRegression(max_iter=2000)\n",
    "lr_clf.fit(X_tr, y_tr)\n",
    "pred_lr = lr_clf.predict(X_val)\n",
    "print(\"LR F1:\", f1_score(y_val, pred_lr, average=\"macro\"))\n",
    "\n",
    "# --------------------\n",
    "# 5) MLPClassifier\n",
    "# --------------------\n",
    "mlp_clf = MLPClassifier(\n",
    "    hidden_layer_sizes=(384, 128),\n",
    "    max_iter=15,\n",
    "    batch_size=128\n",
    ")\n",
    "mlp_clf.fit(X_tr, y_tr)\n",
    "pred_mlp = mlp_clf.predict(X_val)\n",
    "print(\"MLP F1:\", f1_score(y_val, pred_mlp, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniLM + SVC macro-F1: 0.3654851146931038\n",
      "MiniLM + LR macro-F1: 0.3886790072621205\n",
      "MiniLM Ensemble F1: 0.3654851146931038\n"
     ]
    }
   ],
   "source": [
    "weights = {\n",
    "    \"xgb\": 0.30,\n",
    "    \"lgb\": 0.30,\n",
    "    \"svc\": 0.20,\n",
    "    \"lr\":  0.10,\n",
    "    \"mlp\": 0.10\n",
    "}\n",
    "\n",
    "preds = np.vstack([\n",
    "    pred_xgb,\n",
    "    pred_lgb,\n",
    "    pred_svc,\n",
    "    pred_lr,\n",
    "    pred_mlp\n",
    "])\n",
    "\n",
    "def weighted_vote(preds, weights):\n",
    "    weight_list = np.array(list(weights.values()))\n",
    "    final = []\n",
    "    for col in preds.T:\n",
    "        classes, idx = np.unique(col, return_inverse=True)\n",
    "        score = np.bincount(idx, weights=weight_list)\n",
    "        final.append(classes[np.argmax(score)])\n",
    "    return np.array(final)\n",
    "\n",
    "pred_weighted = weighted_vote(preds, weights)\n",
    "print(\"Weighted Ensemble F1:\", f1_score(y_val, pred_weighted, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining on FULL MiniLM embeddings...\n",
      "Saved:\n",
      "submission_minilm_svc.csv\n",
      "submission_minilm_lr.csv\n",
      "submission_minilm_ens.csv\n"
     ]
    }
   ],
   "source": [
    "meta_train = np.vstack([\n",
    "    pred_xgb,\n",
    "    pred_lgb,\n",
    "    pred_svc,\n",
    "    pred_lr,\n",
    "    pred_mlp\n",
    "]).T\n",
    "\n",
    "meta_clf = LogisticRegression(max_iter=1000)\n",
    "meta_clf.fit(meta_train, y_val)\n",
    "pred_stack = meta_clf.predict(meta_train)\n",
    "\n",
    "print(\"Stacking F1:\", f1_score(y_val, pred_stack, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full-train all models\n",
    "lgb_clf.fit(X_train_full, y_train)\n",
    "xgb_clf.fit(X_train_full, y_train)\n",
    "svc_clf.fit(X_train_full, y_train)\n",
    "lr_clf.fit(X_train_full, y_train)\n",
    "mlp_clf.fit(X_train_full, y_train)\n",
    "\n",
    "test_pred = weighted_vote(np.vstack([\n",
    "    xgb_clf.predict(X_test_full),\n",
    "    lgb_clf.predict(X_test_full),\n",
    "    svc_clf.predict(X_test_full),\n",
    "    lr_clf.predict(X_test_full),\n",
    "    mlp_clf.predict(X_test_full),\n",
    "]), weights)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": test_df[\"id\"],\n",
    "    \"emotion\": test_pred\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission_cpu_ensemble.csv\", index=False)\n",
    "print(\"Saved submission_cpu_ensemble.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dm2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
